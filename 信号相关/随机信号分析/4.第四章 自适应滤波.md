# 第四章 自适应滤波

[TOC]

## 4.1自适应滤波器的基本概念

基本概念：

> 根据信号环境特性用递推（自适应）算法，自动调整滤波器的脉冲响应，使自适应滤波的输出尽可能接近期望输出。在平稳环境中，迭代次数充分大时，自适应算法给出的结果在某种统计意义上收敛于维纳解，在非平稳环境中，自适应算法具有一定的跟踪输入数据统计特性（慢）变化的能力。



## 4.2 自适应滤波器的结构

* 空间采样

> $$
> 输入：X(n)=
> \left[
>     \begin{matrix}
>       x_{0}(n)  \\
>       x_{1}(n)  \\
>       \vdots  \\
>       x_{N-1}(n)  \\      
>     \end{matrix}
> \right]  
> \qquad
> 权向量：W(n)=
> \left[
>     \begin{matrix}
>       w_{0}(n)  \\
>       w_{1}(n)  \\
>       \vdots  \\
>       w_{N-1}(n)  \\      
>     \end{matrix}
> \right]
> \\
> 输出：y(n)=X^T(n)W(n)
> $$
>

这里的 $x_i(n)$ 可以看作空间不同位置采集到信号。



* 时间采样

> $$
> 输入：X(n)=
> \left[
>     \begin{matrix}
>       x(n)  \\
>       x(n-1)  \\
>       \vdots  \\
>       x(n-N+1)  \\      
>     \end{matrix}
> \right]  
> \qquad
> 权向量：W(n)=
> \left[
>     \begin{matrix}
>       w_{0}(n)  \\
>       w_{1}(n)  \\
>       \vdots  \\
>       w_{N-1}(n)  \\      
>     \end{matrix}
> \right]
> \\
> 输出：y(n)=X^T(n)W(n)
> $$

这里的 $x(n-i)$ 可以看作同一信号经过延迟单位采集到的信号。



* 空时采样

空时采样是在每个空间采样的分支上加入时间采样。



下面将主要介绍两种自适应算法

1. 随机梯度法（LMS）
2. 最小二乘法（RLS）



## 4.3 LMS 自适应滤波器

LMS 自适应滤波器和维纳滤波器一样，也是以均方误差最小（即 $E(e_j^2)=E[(d_j-y_j)^2]$ 最小）作为其最佳滤波准则的。下面我们推到最小均方误差与自适应滤波的权系数 $W$ 之间的关系。
$$
y_j=X_j^TW=W^TX_j \\
e_j=d_j-y_j=d_j-W^TX_j
$$
所以
$$
E[e_j^2]=E[(d_j-y_j)^2]=E[d_j^2]-2E[d_jX_j^T]W+W^TE[X_jX_j^T]W
$$
令
$$
\begin{aligned}
P&=E[d_jX_j]=E[d_jx_{1j},d_jx_{2j},\cdots,d_jx_{Nj}]^T \\
 &=d_j与X_j的互相关矢量
\end{aligned}  \\
\begin{aligned}
R&=E[X_jX_j^T]=E
\left[
    \begin{matrix}
    x_{1j}x_{1j} &x_{1j}x_{2j} &\cdots &\cdots       \\
    x_{2j}x_{1j} &x_{2j}x_{2j} &\cdots &\cdots       \\
    \cdots       &\cdots       &\cdots &\cdots       \\
    \vdots       &\vdots       &\cdots &x_{Nj}x_{Nj} \\
    \end{matrix}
\right] \\
 &=输入X_j的自相关矩阵
\end{aligned}
$$
于是均方误差可以改写为
$$
E[e_j^2]=E[d_j^2]-2P^TW+W^TRW
$$
对于平稳输入，上式的 $E[e_j^2]$ 是权向量 $W$ 的二次型函数，因此 $E[e_j^2]$ ~ $W$ 是一个凹形超抛物体曲面，它有唯一的极小点。均方误差的梯度可以对上式求导得。
$$
\nabla_j=
\left \{
	\frac{\partial E[e_j^2]}{\partial w_1},
	\frac{\partial E[e_j^2]}{\partial w_2},\cdots,
	\frac{\partial E[e_j^2]}{\partial w_N}
\right \}
=-2P+2RW
$$

若 $\nabla_j=0$ 就可以得到最佳权向量 $w_{opt}$ ，即
$$
-2P+2RW=0 \\
W=W_{opt}=R^{-1}P
$$
最小均方误差为
$$
(E[e_j^2])_{min}=E[d_j^2]-W_{opt}^TP
$$

自适应滤波与维纳滤波相比差别在于它加了一个识别控制环节，将输出的 $y_j$ 与所希望的值 $d_j$ 比较，看是否一样，如果有误差，则用 $e_j$ 去控制 $W$ ，使 $W$ 逐步逼近 $E[e_j^2]=(E[e_j^2])_{min}$ 。因此关键在于怎么简便地寻找 $W_{opt}$ ，LMS 算法则是常用的主要算法之一。

==分析==：$W_{opt}$ 是可以准确地求出来地，但是在权地数目 $N$ 很大或者输入数据率很高时，用这种方法直接求解在计算上会遇到很大困难，它不仅需要计算 $N\times N$ 矩阵的逆，而且还需要测量或估计 $N(N+1)/2$ 这么多个自相关或互相关函数才能得到 $P$ 和 $R$ 的各矩阵元素。不仅如此，当输入量的统计特性发生某种变化后，还必须重新作相应的计算。



### 4.3.1 最陡下降法原理

这里我们使用 $J(w)$ （ $J(w)=E[e_j^2]$ ）代价函数来描述均方误差。
$$
J(w+\Delta w)=J(w)+(\Delta w)^T\nabla J(w)+\frac{1}{2}(\Delta w)^T\nabla^2 J(\Delta w)
$$
要使
$$
J(w+\Delta w) < J(w) ，显然应使(\Delta w)^T\nabla J(w) < 0
$$
因为 $J(w+\Delta w)$ 的第三项显然是正定的。$(\Delta w)^T\nabla J(w)$ 表示的是两个向量的内积，我们换种方式来表示有
$$
(\Delta w)^T\nabla J(w)=|\Delta w|\cdot|\nabla J(w)|\cdot cos\theta
$$
$\theta$ 为两向量的夹角，$cos\theta$ 的取值范围为 `-1~1` 所以当 $\Delta w=-k\nabla J(w)$ ，即 $\Delta w$ 与 $\nabla J(w)$ 方向相反时 $(\Delta w)^T\nabla J(w)$ 下降得越快。基于上面得分析，我们设计 $W(n)$ 的修正方案
$$
W_{j+1}=W_j-\mu\nabla J(w)
$$
$\mu>0$ 称为步长参数，可以控制算法的收敛速度和稳定性。

> 收敛速度：一般来说步长越大下降的速度越快，收敛速度越快
>
> 稳定性：但是超过一定限度，步长过大会导致系统震荡甚至发散，无法收敛。

当 $W$ 为多维时，梯度 $\nabla J$ 可以用列矩阵表示
$$
\nabla J=
\left [
	\frac{\partial J}{\partial W_1},
	\frac{\partial J}{\partial W_2},\cdots,
	\frac{\partial J}{\partial W_N}
\right ]_{W=W_j}=
\left [
	\frac{\partial E[e_j^2]}{\partial w_1},
	\frac{\partial E[e_j^2]}{\partial w_2},\cdots,
	\frac{\partial E[e_j^2]}{\partial w_N}
\right ]^T
$$

将上式的求导与期望次序对换，得
$$
\nabla J=
2E  \left [ e_j
\left(
	\frac{\partial e_j}{\partial w_1},
	\frac{\partial e_j}{\partial w_2},\cdots,
	\frac{\partial e_j}{\partial w_N}
\right )^T 
\right ]
$$
根据
$$
e_j=d_j-W^TX_j
$$
将有
$$
\left(
	\frac{\partial e_j}{\partial w_1},
	\frac{\partial e_j}{\partial w_2},\cdots,
	\frac{\partial e_j}{\partial w_N}
\right )^T 
=-X_j
$$
令 $\nabla J = 0$ ，即 $E[e_jX_i]=0$ 时， $W=W_{opt}$ 以及 $E[e_j^2]=E[e_j^2]_{min}$ 与维纳滤波的理论是一致的。上面，我们假定在自适应滤波的递推过程中，每次迭代所需要的梯度向量是可精确测量的。而实际应用中，为了便于用实时系统实现，往往仅取单个误差样本的平方 $e_j$ 的梯度作为均方误差梯度的估计，并以 $\nabla\widehat{J}$ 表示这个估计，于是有
$$
\nabla\widehat{J}=
\left [
	\frac{\partial e_j^2}{\partial w_1},
	\frac{\partial e_j^2}{\partial w_2},\cdots,
	\frac{\partial e_j^2}{\partial w_N}
\right ]^T
=2 e_j\left [
	\frac{\partial e_j}{\partial w_1},
	\frac{\partial e_j}{\partial w_2},\cdots,
	\frac{\partial e_j}{\partial w_N}
\right ]^T \\
\nabla\widehat{J}=-2e_jX_j
$$
与上式比较，有 $E[\nabla\widehat{J}] = \nabla J=2E[e_jX_j]$ ，即 $\nabla\widehat{J}$ 的期望值等于其真实值  ，故这种对 $\nabla J$ 的估计是无偏估计，即 $\nabla J$ 的估计值 $\nabla\widehat{J}$ 是用 $[e_jX_j]$ 的瞬时值代替它的期望值 $E[e_jX_j]$ 得到的，于是有
$$
W_{j+1}=W_j-\mu\nabla\widehat{J}=W_j+2\mu e_jX_j
$$
其中 $e_j=d_j-W^TX_j$ ，这种算法称为维德罗 — 霍夫最小均方（Widrow-Hoff LMS）算法，这种算法对于每一个输入样本只需要进行两个乘法和两个加法的运算，省略了期望的计算大大缩减计算量。



### 4.3.2 LMS 算法的收敛性质

上一节提到 $\mu$ 是一个很关键的参数，影响算法的收敛速度和稳定性，那么如何确定参数 $\mu$ 的值是一个值得探讨的问题。下面我们将讨论这个问题。

假设在两次递推之间有充分大的时间间隔，即认为二次输入信号 $X_{j}$ 与 $X_{j+1}$ 是不相关的，即 
$$
E[X_jX_{j+1^T}]=0,\quad 当l \neq 0
$$
同时，由于 $W_j$ 仅是输入 $X_{j-1},X_{j-2},\cdots,X_{0}$ 的函数，故 $W_j$ 与 $X_j$ 也不相关
$$
W_{j+1}=W_j+2\mu[e_jX_j]=W_j+2\mu\{X_j[d_j-X_j^TW_j]\}
$$
因为 $W_j$ 是随机变量，我们必须利用它的集合平均，即
$$
E[W_{j+1}]=E[W_j]+2\mu E[d_jX_j]-2\mu E[X_jX_j^TW_j]
$$
考虑到 $W_j$ 与 $X_j$ 也不相关，令 $P=E[d_jX_j],R=E[X_jX_j^T]$ 有
$$
E[W_{j+1}]=[I-2\mu R]E[W_j]+2\mu P
$$
递推起来有
$$
E[W_{j+1}]=[I-2\mu R]^{j+1}E[W_0]+2\mu\sum_{i=0}^j [I-2\mu R]^{i}P
$$
这里自相关矩阵 $R$ 是实对称阵，我们可以写出其特征值分解式 $R=Q\Lambda Q^T$ ，$Q$ 是自相关矩阵 $R$  的正交矩阵，并有 $QQ^T=I,Q^T=Q$ ，$\Lambda$ 是由 $R$ 的特征值组成的对角矩阵
$$
\Lambda=
\left [
 \begin{matrix}
 \lambda_1  &\quad      &\quad   &\quad     \\
 \quad      &\lambda_2  &\quad   &0         \\
 \quad      &\quad      &\vdots  &\quad     \\
 0          &\quad      &\quad   &\lambda_N \\
 \end{matrix}
\right ]
$$

继续简化上式
$$
\begin{aligned}
E[W_{j+1}]&=[I-2\mu R]^{j+1}E[W_0]+2\mu\sum_{i=0}^j [I-2\mu R]^{i}P \\
          &=[I-2\mu Q\Lambda Q^{-1}]^{j+1}E[W_0]
            +2\mu\sum_{i=0}^j [I-2\mu Q\Lambda Q^{-1}]^{i}P
\end{aligned} \\
$$
另外，因为
$$
\begin{aligned}[]
[I-2\mu Q\Lambda Q^{-1}]^{j}&=(QQ^{-1}-2\mu Q\Lambda Q^{-1})^j \\
                            &=[Q(I-2\mu\Lambda)Q^{-1}]^j \\
                            &=Q(I-2\mu\Lambda)^jQ^{-1}
\end{aligned}
$$
所以有
$$
E[W_{j+1}]=Q[1-2\mu\Lambda]^{j+1}Q^{-1}E[W_0]
           +2\mu Q\sum_{i=0}^j [I-2\mu\Lambda]^iQ^{-1}P
$$
为了使上式收敛，必须满足 $|1-2\mu\lambda_{max}|<1$ ，即 $0<\mu<\frac{1}{\lambda_{max}}$ ，当 $j\rightarrow \infty$ 时，算法法收敛，这时第一项为零
$$
\lim\limits_{j\rightarrow\infty}E[W_{j+1}]=2\mu Q\lim\limits_{j\rightarrow\infty}\sum_{i=0}^j [I-2\mu\Lambda]^iQ^{-1}P
$$
有无限级数可知
$$
\lim\limits_{j\rightarrow\infty}\sum_{i=0}^j [I-2\mu\Lambda]^i=\frac{1}{I-(I-2\mu\Lambda)}=\frac{1}{2\mu}\Lambda^{-1}
$$
所以
$$
\lim\limits_{j\rightarrow\infty}E[W_{j+1}]
	=2\mu Q[\frac{1}{2\mu}\Lambda^{-1}]Q^{-1}P
	=R^{-1}P
	=W_{opt}
$$
通常 $\lambda_{max}$ 是我们预先不知道的，而 $tr[R]=\sum_iE[X_i^2]=\sum_i\lambda_i$ 是我们容易知道的，且 $R$ 是正定的，所以 $\mu$ 的取值也可以保守取为 $0<\mu<\frac{1}{tr[R]}$ ，上式中的 $\sum_iE[X_i^2]$ 为信号的总输入功率，一般是已知的。

* 学习曲线

一般我们把代价函数的 $J(W_j)$ 与迭代次数 $j$ 的曲线 $J(W_j)$ ~ $j$ 叫做学习曲线，即任意一个初始权向量 $W_0$ 通过一次次递归都能趋近于 $W_{opt}$ 。一般曲线会呈现为指数形式，因为在一维情况下，$\Delta W=2\mu X_j^2\Delta W_j$ ，即 $\Delta W$ 正比于 $\Delta W_j$ 。

下面我们对代价函数进行 “简化”

首先我们对 $W$ 先平移再旋转

> 平移： $\widetilde{W}=W-W_{opt}$ 
>
> 旋转： $V=Q^T\widetilde{W}$ 

$$
\begin{aligned}
J(W_j)&=J_{min}(W)-2P^TW+W^TRW-W_{opt}^TP \\
      &=J_{min}(W)+(W-W_{opt})^TR(W-W_{opt}) \\
      &=J_{min}(W)+V_j^T\Lambda V_j
\end{aligned}
$$

$J(W)$ 对 $W$ 微分 
$$
\begin{aligned}
\nabla J&=-2P+2RW \\
	    &=-2RW_{opt}+2RW\\
	    &=2R\widetilde{W}
\end{aligned}
$$
 $W$ 的递推公式
$$
W_{j+1}=W_j-\mu \nabla J \\
W_{j+1}-W_{opt}=W_j-W_{opt}-\mu \nabla J \\
\widetilde{W}_{j+1}=\widetilde{W}_{j}-2\mu R\widetilde{W}_{j} \\
Q^{-1}\widetilde{W}_{j+1}=Q^{-1}Q[I-2\mu\Lambda]Q^{-1}\widetilde{W}_{j}\\
V_{j+1}=[I-2\mu\Lambda]V_{j} \\
V_{j}=[I-2\mu\Lambda]^jV_{0}
$$
所以
$$
J(W_j)=J_{min}(W)+V_0^T\Lambda[I-2\mu\Lambda]^{2j}V_0
$$
每一次迭代，等式右边第二项衰减 $[I-2\mu\Lambda]^{2j}$ 倍，当 $j\rightarrow\infty$ ，$J(W)=J_{min}(W)$ 。而且， $E[e_j^2]$ 随着 $j$ 增加的衰减比 $W_j$ 快一倍。



如果权向量 $W$ 有 $N$ 个分量，则有 $N$ 个自由度和 $N$ 个特征值 $\lambda$ ，同时有 $N$ 个特征振动模式，第 $p$ 个振动模式的均方误差能量每次衰减 $[1-2\mu\lambda_p]^{2j}$ 。定义 $\gamma_p^2 \triangleq (1-2\mu\lambda_p)^{2}$ $(\gamma_p^2)^j$ 随迭代次数 $j$ 做指数衰减。
$$
\gamma_p = e^{-\frac{1}{\tau_p}}=1-\frac{1}{\tau_p}+\frac{1}{2!\tau_p^2}+\cdots
$$
对于通常比较平稳且缓慢的自适应过程（$\mu$ 相当小），$\tau_p$ 一般比较大，因此可以忽略上式右边的高次项，即
$$
\gamma_p =1-2\mu\lambda_p \approx 1-\frac{1}{\tau_p}  \\
\tau_p \approx \frac{1}{2\mu\lambda_p}
$$
对于均方误差来讲，其衰减时间常数以 $\tau_{msc}$ 表示（对平均超量的衰减时间常数 $\tau_{msc}=\tau_p/2$ ，即它为 $W_j$ 的衰减时间的 $1/2$  ）.在各个振动模式的特征值均相等，并且等于 $\lambda$ 的特殊情况下，各振动模式的时间常数也相等，为
$$
\tau_{msc}=\frac{1}{4\mu\lambda}
$$
所以同时有
$$
\tau_{max}=\frac{tr[R]}{2\lambda_{min}}
$$
这里我们分析 LSM 算法的均方误差
$$
J(n)=J_{min}+J_{tr}(n)+J_{ex}(\infty)
$$

> 其中
>
> $J_{tr}(n)$ 是瞬态误差， $J_{tr}(n=\infty)=0$ ；
>
> $J_{ex}(\infty)$ 是稳态超量误差，是由于梯度噪声引起的

在稳态时 $J(\infty)=J_{min}+J_{ex}(\infty)>J_{min}$ 性能变差，为次优解。

> $\mu$ 很小时，$J_{ex}(\infty) \approx \mu J_{min}\cdot tr[R]$ ;
>
> $\mu$ 越小，超量误差越小，收敛速度越慢
>
> 失调量 $M=\frac{J_{ex}(\infty)}{J_{min}}\approx tr[R]\cdot \mu$ 



 

## 4.4 递推最小二乘算法 RLS



基本的 RLS 自适应算法所遵循的准则是确定这样的 $W$ ，它使
$$
e(k)=d(k)-y(k)=d(k)-W^TX(k)
$$
（权向量在观测区间 $1 \leq i \leq n$ 保持不变）误差的加权和
$$
\varepsilon(k,W)=\sum_{i=1}^{k}\lambda^{k-i}|e(i)|^2
			 =\sum_{i=1}^{k}\beta(k,i)|e(i)|^2  \\
\beta(k,i)=\lambda^{k-i},\quad0<\lambda<1
$$
 $\lambda$ 被称为遗忘因子， $\lambda=1$ 时相当于无限记忆（也就是没有跟踪能力）；当 $\lambda<1$ 时（指数加权）遗忘过去的数据，使算法具有跟踪新数据的能力，$\frac{1}{1-\lambda}$ 为记忆长度的度量（有效数据）。 

为了使加权平方和最小，我们令 $\varepsilon(k,W)$ 对 $W$ 的偏导数为 0
$$
\begin{aligned}
\frac{\partial \varepsilon(k,W)}{\partial W}
    &=\frac{\partial}{\partial W}\sum_{i=1}^{k}\lambda^{k-i}[d(k)-W^TX(k)]^2  \\
    &=-2\sum_{i=1}^{k}\lambda^{k-i}[d(k)-W^TX(k)]X^T(i) \\
    &=0
\end{aligned}
$$




































